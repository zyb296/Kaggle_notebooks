#### 参考：

- [Machine Learning for Insights Signup](https://www.kaggle.com/ml-for-insights-signup)
- 

## 1. 删除线性关联过高的特征



## 2. LightGBM特征重要性选择



## 3. 对抗验证进行特征选择

### 3.1 原理

将训练集和测试集进行合并，给测试集一个label=1，训练集一个label=0，进行训练。采用AUC作为衡量指标，AUC小于0.7为佳。

### 3.2 优缺点

#### 3.2.1 缺点：

- 通常在训练集和测试集的分布相似的情况下才能有效检测到在训练集中有效但在测试集中无效的特征
- 

### 3.3 代码



## 4. NUll-Importance特征选择

### 4.1 原理

- 将某个特征打乱，并用打乱后的特征拟合正确的目标，并以一定的标准衡量特征的重要程度（通过为lightgbm，xgboost的特征重要性指标）
- 如果打乱后的特征的重要性反而比未打乱的特征更重要，则该特征无效
- 可用shuffle标签的方式代替shuffle特征值来达到同样的效果

### 4.2 优缺点

### 4.3 代码

### 4.4 参考

- [Feature Selection with Null Importances](https://www.kaggle.com/ogrellier/feature-selection-with-null-importances)
- 

## 5. Permutation Importance

### 5.1 原理

- 训练好模型
- 取一个feature，打乱顺序，用模型重新

### 5.2 优缺点

##### 缺点：

- 会受到**特征自相关**的影响，shuffle一个特征后，其他高相关性的特征会提供相同的信息，无法真实反应permutation的效果。

### 5.3 代码实现

```python
import eli5
from eli5.sklearn import PermutationImportance

perm = PermutationImportance(my_model, random_state=1).fit(val_X, val_y)
eli5.show_weights(perm, feature_names = val_X.columns.tolist())
```

### 5.4 参考

- [特征重要性排序--Permutation Importance](https://blog.csdn.net/qian_chun_qiang/article/details/115323852)
- [机器学习中常用的特征选择方法及非常详细的Python实例](https://zhuanlan.zhihu.com/p/141010878)
- [Permutation Importance](https://www.kaggle.com/dansbecker/permutation-importance)

## 6. Boruta

##### 参考：

- [Automated feature selection with boruta](https://www.kaggle.com/residentmario/automated-feature-selection-with-boruta)
- [源码](https://github.com/scikit-learn-contrib/boruta_py)

### 6.1 原理

- 对特征矩阵X的各个特征取值进行shuffle, 将shuffle后的特征(shadow features)与原特征(real features)拼接构成新的特征矩阵。
- 使用新特征矩阵作为输入, 训练可以输出feature_importance的模型。
- 计算real feature和shadow feature的Z_score。
- 在shadow features中找出最大的Z_score记为$Z_{max}$
- 将Z_socre大于的real feature标记为"重要"，将Z_score显著小于的real feature标记为"不重要"，并且从特征集合中永久剔除。
- 删除所有shadow features.
- 重复1~6，直到所有特征都被标记为"重要"或者"不重要"

单个特征的$Z_{score}$的计算方式：
$Z_{score}=a verage_feature_importance / feature_importance$的标准差

### 6.2 优缺点

### 6.3 代码
